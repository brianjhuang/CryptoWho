{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from config import reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class YoutubeLink:\n",
    "    url: str\n",
    "    subreddit: str\n",
    "    thread_url: str\n",
    "    \n",
    "@dataclass\n",
    "class TextEntry:\n",
    "    text: str\n",
    "    subreddit: str\n",
    "    thread_url: str\n",
    "    text_type: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_subreddit(subreddit):\n",
    "    \"\"\"Gets thread links from a subreddit, up to a specified depth\n",
    "\n",
    "    Args:\n",
    "        subreddit (str): Name of subreddit to scrape\n",
    "\n",
    "    Returns:\n",
    "        list: List of all thread links found\n",
    "    \"\"\"\n",
    "    url = f\"https://old.reddit.com/r/{subreddit}/top/?sort=top&t=year\"\n",
    "    driver.get(url)\n",
    "    threads = []\n",
    "    for i in range(SCRAPE_DEPTH):\n",
    "        # Get all thread links\n",
    "        soup = BeautifulSoup(driver.page_source)\n",
    "        links = soup.find_all('a', {\"class\": \"title\"})\n",
    "        threads += [link for link in links if link['href'].startswith('/r/')]\n",
    "        \n",
    "        driver.find_element(By.CLASS_NAME, 'next-button').click()\n",
    "\n",
    "    return threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper():\n",
    "    def __init__(self, scrape_depth = reddit.SCRAPE_DEPTH, subreddits = reddit.SUBREDDITS):\n",
    "        self.SCRAPE_DEPTH = scrape_depth\n",
    "        self.SUBREDDITS = subreddits\n",
    "        self.YoutubeLinks = []\n",
    "        self.TextEntries = []\n",
    "    \n",
    "        self.driver = webdriver.Firefox()\n",
    "        \n",
    "    def extract_subreddit(self, url):\n",
    "        \"\"\"Gets the subreddit from a URL\n",
    "\n",
    "        Args:\n",
    "            url (str): URL to extract subreddit from\n",
    "        \"\"\"\n",
    "        subreddit = re.findall(r\"/r/(\\w+)/\", url)[0]\n",
    "        return subreddit\n",
    "    \n",
    "    def process_thread(self, url):\n",
    "        \"\"\"Scrapes a thread, saving the thread text, comment text, and any youtube links found\n",
    "\n",
    "        Args:\n",
    "            url (str): The url of the thread to scrape\n",
    "        \"\"\"\n",
    "        \n",
    "        # old.reddit.com is easier to scrape, doesn't have js lazy loading\n",
    "        self.driver.get(url)\n",
    "        soup = BeautifulSoup(self.driver.page_source)\n",
    "        \n",
    "        subreddit = self.extract_subreddit(url)\n",
    "        \n",
    "        main_thread = soup.find('div', {'class': 'expando'}).text\n",
    "        self.TextEntries.append(TextEntry(main_thread, subreddit, url, 'thread'))\n",
    "        \n",
    "        #Find all links in comments, keeping only youtube links\n",
    "        comments = soup.find_all('div', {\"data-type\": \"comment\"})\n",
    "        for comment in comments:\n",
    "            #Record comment text\n",
    "            comment_text = comment.find('div', {'class': 'md'}).text\n",
    "            self.TextEntries.append(TextEntry(comment_text, subreddit, url, 'comment'))\n",
    "            \n",
    "            #Record youtube links\n",
    "            for link in comment.find('div', {'class': 'usertext-body'}).find_all('a', href=True):\n",
    "                if 'youtube.com' in link['href']:\n",
    "                    self.YoutubeLinks.append(YoutubeLink(link['href'], subreddit, url))\n",
    "                    \n",
    "    def process_subreddit(self, subreddit):\n",
    "        \"\"\"Gets thread links from a subreddit, up to a specified depth\n",
    "\n",
    "        Args:\n",
    "            subreddit (str): Name of subreddit to scrape\n",
    "\n",
    "        Returns:\n",
    "            list: List of all thread links found\n",
    "        \"\"\"\n",
    "        url = f\"https://old.reddit.com/r/{subreddit}/top/?sort=top&t=year\"\n",
    "        self.driver.get(url)\n",
    "        threads = []\n",
    "        for i in range(self.SCRAPE_DEPTH):\n",
    "            # Get all thread links\n",
    "            soup = BeautifulSoup(self.driver.page_source)\n",
    "            links = soup.find_all('a', {\"class\": \"title\"})\n",
    "            threads += [link for link in links if link['href'].startswith('/r/')]\n",
    "            \n",
    "            self.driver.find_element(By.CLASS_NAME, 'next-button').click()\n",
    "\n",
    "        return threads\n",
    "    \n",
    "    def scrape(self):\n",
    "        \"\"\"Scrapes all subreddits, saving all youtube links and text entries found\n",
    "        \"\"\"\n",
    "        for subreddit in self.SUBREDDITS:\n",
    "            threads = self.process_subreddit(subreddit)\n",
    "            for thread in threads:\n",
    "                thread_url = f\"https://old.reddit.com{thread['href']}\"\n",
    "                self.process_thread(thread_url)\n",
    "                \n",
    "        self.driver.close()\n",
    "                \n",
    "    def save(self):\n",
    "        pd.DataFrame(self.YoutubeLinks).to_csv(reddit.RAW_THREADS + 'scraped_youtube_links.csv', index=False)\n",
    "        pd.DataFrame(self.TextEntries).to_csv(reddit.RAW_THREADS + 'scraped_comments.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Scraper()\n",
    "s.scrape()\n",
    "s.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "bc204a9723bb08316d50099ac13d40ce960bbd8b2366046bd948bed845217909"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
