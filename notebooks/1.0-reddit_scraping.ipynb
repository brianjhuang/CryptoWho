{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from config import reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class YoutubeLink:\n",
    "    url: str\n",
    "    subreddit: str\n",
    "    thread_url: str\n",
    "    \n",
    "@dataclass\n",
    "class Comment:\n",
    "    comment_text: str\n",
    "    subreddit: str\n",
    "    thread_url: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBREDDITS = reddit.SUBREDDITS\n",
    "YoutubeLinks = []\n",
    "Comments = []\n",
    "\n",
    "driver = webdriver.Firefox()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_thread(url, subreddit):\n",
    "    # old.reddit.com is easier to scrape, doesn't have js lazy loading\n",
    "    driver.get(url)\n",
    "    soup = BeautifulSoup(driver.page_source)\n",
    "    \n",
    "    #Find all links in comments, keeping only youtube links\n",
    "    comments = soup.find_all('div', {\"data-type\": \"comment\"})\n",
    "    for comment in comments:\n",
    "        #Record comment text\n",
    "        comment_text = comment.find('div', {'class': 'md'}).text\n",
    "        Comments.append(Comment(comment_text, subreddit, url))\n",
    "        \n",
    "        #Record youtube links\n",
    "        for link in comment.find('div', {'class': 'usertext-body'}).find_all('a', href=True):\n",
    "            if 'youtube.com' in link['href']:\n",
    "                YoutubeLinks.append(YoutubeLink(link['href'], subreddit, url))\n",
    "                print(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"https://m.youtube.com/watch?v=Akm7ik-H_7U\">mandatory parking minimums</a>\n"
     ]
    }
   ],
   "source": [
    "for SUBREDDIT in SUBREDDITS[:1]:\n",
    "    url = f\"https://reddit.com/r/{SUBREDDIT}/top/?sort=top&t=year\"\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Get all thread links\n",
    "    soup = BeautifulSoup(driver.page_source)\n",
    "    links = soup.find_all('a', {\"data-click-id\": \"body\"})\n",
    "    threads = [link for link in links if link['href'].startswith('/r/')]\n",
    "    \n",
    "    for thread in threads[:5]:\n",
    "        thread_url = f\"https://old.reddit.com{thread['href']}\"\n",
    "        links = process_thread(thread_url, SUBREDDIT)\n",
    "                \n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(YoutubeLinks).to_csv('../data/raw/reddit/scraped_youtube_links.csv', index=False)\n",
    "pd.DataFrame(Comments).to_csv('../data/raw/reddit/scraped_comments.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e5e9da20ac624690ae763919769c043ee092f8509a0b80cd8ab242b626dc7799"
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "a7f8db6aea59d1f10a9d3a436b54c50bfeaa0a65429587619fcd16b6d9d618cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
