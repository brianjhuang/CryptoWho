{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim.summarization'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummarization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summarize\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim.summarization'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import openai\n",
    "from gensim.summarization import summarize\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/seed/youtube/seed_videos_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyTfidf(corpus, n):\n",
    "    \"\"\"Apply TF-IDF\"\"\"\n",
    "    # Should I just make one of these?\n",
    "    # Convert the text into a sparse matrix using TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf = vectorizer.fit_transform([corpus])\n",
    "    \n",
    "    # Get the feature names and scores\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    scores = dict(zip(feature_names, tfidf.data))\n",
    "    \n",
    "    # Sort the scores in descending order\n",
    "    sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Select only the top N features\n",
    "    selected_features = [x[0] for x in sorted_scores[:n]]\n",
    "    \n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topTags(tags, n = 10):\n",
    "    \"\"\"Apply TF-IDF to the tags to get the ten most relevant tags\"\"\"\n",
    "    corpus = \" \".join([tag.strip().strip(\"\\'\") for tag in tags[1:-1].strip('[').strip(']').split(',')])\n",
    "    \n",
    "    selected_features = applyTfidf(corpus, n)\n",
    "    \n",
    "    return \" \".join(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def condenseTranscript(transcript, summary = False, tfidf = False, word_count = 100, n = 50):\n",
    "    \"\"\"Cleans and condenses transcript\"\"\"\n",
    "    corpus = transcript.replace(\"\\n\", \" \").replace(\" - \", \"\").replace('- ', \"\").replace(\"\\'\", \"\").replace(\".\", \". \")\n",
    "    if summary:\n",
    "        if len(corpus) < word_count:\n",
    "            return corpus\n",
    "        elif len(corpus.split(\".\")) <= 1:\n",
    "            return corpus\n",
    "        return summarize(corpus, word_count = word_count)\n",
    "    elif tfidf:\n",
    "        if len(corpus) < word_count:\n",
    "            return corpus\n",
    "        elif len(corpus.split(\".\")) <= 1:\n",
    "            return corpus\n",
    "        return \" \".join(applyTfidf(corpus, n)) + \".\"\n",
    "    else:\n",
    "        return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVideoSnippet(title, transcript, tags):\n",
    "    \"\"\"Creates the video snippet\"\"\"\n",
    "    return title + '. ' + transcript.replace(\"\\n\", \" \") + ' ' + tags + \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createPrompt(videos):\n",
    "    \"\"\"Create prompt given video snippet list\"\"\"\n",
    "#     prompt=\"These labels determine if a video is discussing or recommending the following investments:\\nAlternative: Cryptocurrency, Blockchain, NFTs\\nTraditional: Stocks, Bonds, Real Estate, Commodities\\nMixed: Discusses at least one of each topic from alternative and traditional labels defined above.\\nNone: Not related to investing or finance\\n\\nClassify these YouTube video snippets, with each snippet containing the title, transcript, and video tags.\\n\\nFor each video return probabilities for all of the labels and explain the most probable label.\\n\\nExample Output:\\nAlternative: 0.15 Traditional 0.15 Mixed .1 None .6 None because lorem ipsum.\"\n",
    "#     prompt=\"These labels determine if a video is discussing or recommending the following investments:\\nAlternative: Cryptocurrency, Blockchain, NFTs\\nTraditional: Stocks, Bonds, Real Estate, Commodities\\nMixed: Must at least one of each topic from alternative and traditional labels defined above.\\nNone: Doesn't discuss the topics above or related topics.\\n\\nFor each video return probabilities for all of the labels and explain the most probable label.\\nExample Output:\\nAlternative: 0.15 Traditional 0.15 Mixed .1 None .6 None because lorem ipsum.\\n\\nClassify each YouTube video snippet below, with each snippet containing the title, transcript, and video tags.\"\n",
    "    prompt=\"These labels determine if a video is discussing or recommending the following investments:\\n1. Alternative: Cryptocurrency, Blockchain, NFTs\\n2. Traditional: Stocks, Bonds, Real Estate, Commodities\\n3. Mixed: Discusses at least one of each topic from alternative and traditional labels\\n4. None: Not related to investing or finance\\n\\nClassify these YouTube video snippets, with each snippet containing the title, transcript, tags.\"\n",
    "    for i in range(len(videos)):\n",
    "        prompt += '\\n\\n{} '.format(i+1) + videos[i]\n",
    "    prompt += \"\\n\\nFor each video return probabilities for all of the labels and explain the most probable label.\\nExample Output:\\nAlternative: 0.15 Traditional: 0.15 Mixed: .1 None: .6 None because lorem ipsum\"\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the API client\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make classification requests\n",
    "def classify(prompt, model_engine = \"text-davinci-002\", max_tokens = 1024, n = 1, temperature = 0.5):\n",
    "    completions = openai.Completion.create(\n",
    "        engine=model_engine,\n",
    "        prompt=prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        n=n,\n",
    "        stop=None,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    return completions.choices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos.append(createVideoSnippet(df['title'][0], condenseTranscript(df['cleaned_transcript'][0], summary = True), topTags(df['tags'][0])))\n",
    "# videos.append(createVideoSnippet(df['title'][0], condenseTranscript(df['cleaned_transcript'][0], tfidf = True), topTags(df['tags'][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = createPrompt(videos)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = classify(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prediction.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = []\n",
    "videos = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    title = df['title'][i]\n",
    "    tags = df['tags'][i]\n",
    "    transcript = df['cleaned_transcript'][i]\n",
    "    \n",
    "    if len(videos) >= 5:\n",
    "        videos = []\n",
    "        prompt = createPrompt(videos)\n",
    "        prompts.append(prompt)\n",
    "        \n",
    "    videos.append(createVideoSnippet(title, topTags(tags), condenseTranscript(transcript, summary = True)))\n",
    "    \n",
    "prompt = createPrompt(videos)\n",
    "prompts.append(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "for prompt in prompts:\n",
    "    predictions.append(classify(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e5e9da20ac624690ae763919769c043ee092f8509a0b80cd8ab242b626dc7799"
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
